# Metrica Vox Machina
> **Gold Standard Computational Linguistics Evaluation for AI**

**Note**: While the system is confirmed to meet 100% of focus-group led requirements, it was time and scope limited as 26,000+ words had to be simultaneously authored for the accompanying thesis, crucial for the fulfillment of the Master of Science (Computer Science) degree schema.

For university submission and mark tracing purposes, the code has been condensed into a single file. Originally, the project's `tkinter` functionality and package-based logic spanned across 20+ files. A comprehensive, web-based version of Metrica Vox Machina is slated for release. The full project code will be uploaded after mark ratification.

🌍 **Location**: Birmingham, UK  
🏛 **Institution**: University of Birmingham  
📅 **Duration**: May 2023 - Sep 2023

## 📌 Overview
The recent surge in the adoption of large language models, especially post-GPT-3, underscores the need for standardized, user-friendly evaluation tools. "Metrica Vox Machina" rises to this challenge, offering a suite of evaluative tools designed to rigorously assess the performance of these language models across varied chat-based domains and functionalities.

## 📃 Abstract
The recent meteoric rise in adoption of large language models, particularly since the launch of GPT-3, 
have catalysed the creation of a spectrum of competing models each boasting unique specialities 
derived from their respective training corpora. With increasing heterogeneity of models contributing 
to a growing complexity for the evaluation of these language models, this study addresses the 
pronounced need for an evaluative system with a standardised, user-friendly approach to testing a 
breadth of language models for cross-industrial functionality as a contribution to AI safety. 

The report contains the rational basis, design and implementation for the LLM evaluation system “Metrica Vox 
Machina”, a tool embodying a synthesis of empirically validated and industrially deployed evaluative 
methods. Three standardised suites of evaluation have been developed; firstly, a bidirectional, 50-
language supported machine translation tool, secondly, a cross-comparative human evaluation tool 
with a JSON-validating dataset manager and finally, a contextual embedding tool with smart CUDA 
core threading and format-specific data validation. Continuously evaluated weekly during and post implementation periods, 
the system conclusively meets and exceeds its initial requirements, proving to be a highly effective 
tool for its intended incrementally designed use, the continuous evaluation of modern AI. 

## 🗝 Keywords
`Machine Translation` `Metrics` `Human Evaluation` `Contextual Embedding` `Large Language Models` `AI` `Benchmarking` `Natural Language Processing` `Computational Linguistics` `AGILE` `Confidence Interval` `Focus Group` `User-Friendly` `Decision Making` `Precision` `Recall` `F1` `QA` `Python`.

## 🏆 Achievements
- 💻 Authored **3000+ lines** of Python code, integrating **41+ libraries**.
- 🌐 Developed a **50-language bi-directional Machine Translation system** optimized with BLEU, ROUGE, METEOR, and BERT.
- 🎮 Leveraged gamification techniques, resulting in a **500% surge** in user-initiated data entries.
- 🔄 Conducted **10 AGILE testing phases** with 42 focus group participants, meeting **100%** of **90+ requirements**.

## 🙏 Acknowledgements
Gratitude to:
- **Dr. Jizheng Wan**, my project supervisor, for his astute career guidance and willingness to enable free exploration into the NLP domain. Further projects are planned in NLP thanks to his general stewardship.
- **Dr. Rajesh Chitnis**, my project inspector, for allocating his time to receive the demonstration and emphasising rigorous documentation to "do justice" (in terms of academic awardings) to the project.
- The **43 focus group members** whose contributions were instrumental in refining the system within the limited 10 weeks for implementation.

---
