# Metrica Vox Machina
> **Gold Standard Computational Linguistics Evaluation for AI**

**Note**: While the system is confirmed to meet 100% of focus-group led requirements, it was time and scope limited as 26,000+ words had to be simultaeneously authored for the accompanying thesis, crucial for the fulfillment of the Master of Science (Computer Science) degree schema.

For university submission and mark tracing purposes, the code has been condensed into a single file. Originally, the project's `tkinter` functionality and package-based logic spanned across 20+ files. A comprehensive, web-based version of Metrica Vox Machina is slated for release. The full project code will be uploaded after mark ratification.

ğŸŒ **Location**: Birmingham, UK  
ğŸ› **Institution**: University of Birmingham  
ğŸ“… **Duration**: May 2023 - Sep 2023

## ğŸ“Œ Overview
The recent surge in the adoption of large language models, especially post-GPT-3, underscores the need for standardized, user-friendly evaluation tools. "Metrica Vox Machina" rises to this challenge, offering a suite of evaluative tools designed to rigorously assess the performance of these language models across varied chat-based domains and functionalities.

## ğŸ“ƒ Abstract
The recent meteoric rise in adopÆŸon of large language models, parÆŸcularly since the launch of GPT-3, 
have catalysed the creaÆŸon of a spectrum of compeÆŸng models each boasÆŸng unique specialiÆŸes 
derived from their respecÆŸve training corpora. With increasing heterogeneity of models contribuÆŸng 
to a growing complexity for the evaluaÆŸon of these language models, this study addresses the 
pronounced need for an evaluaÆŸve system with a standardised, user-friendly approach to tesÆŸng a 
breadth of language models for cross-industrial funcÆŸonality as a contribuÆŸon to AI safety. This study 
contains the raÆŸonal basis, design and implementaÆŸon for the LLM evaluaÆŸon system â€œMetrica Vox 
Machinaâ€, a tool embodying a synthesis of empirically validated and industrially deployed evaluaÆŸve 
methods. Three standardised suites of evaluaÆŸon have been developed; firstly, a bidirecÆŸonal, 50-
language supported machine translaÆŸon tool, secondly, a cross-comparaÆŸve human evaluaÆŸon tool 
with a JSON-validaÆŸng dataset manager and finally, a contextual embedding tool with smart CUDA 
core threading and format-specific data validaÆŸon. ConÆŸnuously evaluated weekly during and post implementation periods, the system conclusively meets and exceeds its iniÆŸal requirements, proving 
to be a highly effecÆŸve tool for its intended incrementally designed use, the conÆŸnuous evaluaÆŸon of 
modern AI. 

## ğŸ— Keywords
`Machine Translation` `Metrics` `Human Evaluation` `Contextual Embedding` `Large Language Models` `AI` `Benchmarking` `Natural Language Processing` `Computational Linguistics` `AGILE` `Confidence Interval` `Focus Group` `User-Friendly` `Decision Making` `Precision` `Recall` `F1` `QA` `Python`.

## ğŸ† Achievements
- ğŸ’» Authored **3000+ lines** of Python code, integrating **41+ libraries**.
- ğŸŒ Developed a **50-language bi-directional Machine Translation system** optimized with BLEU, ROUGE, METEOR, and BERT.
- ğŸ® Leveraged gamification techniques, resulting in a **500% surge** in user-initiated data entries.
- ğŸ”„ Conducted **10 AGILE testing phases** with 42 focus group participants, meeting **100%** of **90+ requirements**.

## ğŸ™ Acknowledgements
Gratitude to:
- **Dr. Jizheng Wan**, my project supervisor, for his astute career guidance and willingness to enable free exploration into the NLP domain. Further projects are planned in NLP thanks to his general stewardship.
- **Dr. Rajesh Chitnis**, my project inspector, for allocating his time to receive the demonstration and emphasising rigorous documentation to "do justice" (in terms of academic awardings) to the project.
- The **43 focus group members** whose contributions were instrumental in refining the system within the limited 10 weeks for implementation.

---
